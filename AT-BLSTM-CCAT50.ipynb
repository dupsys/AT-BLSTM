{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mX6E1CSsB01"
   },
   "source": [
    "# AT-BLSTM for CCAT50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsEtBpsYsB03"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQqlnnSoa9Mc"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pydot\n",
    "import itertools\n",
    "import h5py\n",
    "import keras.callbacks\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from AdamW import AdamW\n",
    "from SGDW import SGDW\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Activation, Dropout, Flatten, merge\n",
    "from keras.models import Model\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers import Conv1D, MaxPooling1D,AveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.layers import TimeDistributed, Bidirectional, Lambda\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nltk \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mv_hlCFzsB1I"
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "maxlen = 512\n",
    "max_sentences = 15\n",
    "w_l2 = 1e-4\n",
    "nb_classes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1YqHgc1sB1K"
   },
   "outputs": [],
   "source": [
    "def preProcessText(text):\n",
    "    #str(tweet.encode('utf-8')) \n",
    "    str(text)\n",
    "    #Replace all words preceded by '@' with 'USER_NAME'\n",
    "    text  = re.sub(r'@[^\\s]+', 'USER_NAME', text)  \n",
    "    #Replace all url's with 'URL'\n",
    "    text  = re.sub(r'www.[^\\s]+ | http[^\\s]+',' URL ', text)\n",
    "    #Replace all hashtags with the word\n",
    "    text  = text.strip('#')\n",
    "    #Replace words with long repeated characters with the shorter form\n",
    "    text  = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    #Remove any extra white space\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FkbTLzg3sB1N"
   },
   "outputs": [],
   "source": [
    "def striphtml(s):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub(\"\", str(s))\n",
    "def clean(s):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset now\n",
    "datasource = '../../data/ccat50/'\n",
    "ccat50_train = pd.read_csv(datasource + 'cleantrainaugmented.csv')\n",
    "ccat50_test = pd.read_csv(datasource + 'cleantest.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china has moved step further from its recent r...</td>\n",
       "      <td>JaneMacartney</td>\n",
       "      <td>china has proceed step further from its recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the family of detained chinese dissident wang ...</td>\n",
       "      <td>JaneMacartney</td>\n",
       "      <td>the family of detained chinese dissident wang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the watchdogs of china ruling communist party ...</td>\n",
       "      <td>JaneMacartney</td>\n",
       "      <td>the watchdogs of china find communist party ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          label  \\\n",
       "0  china has moved step further from its recent r...  JaneMacartney   \n",
       "1  the family of detained chinese dissident wang ...  JaneMacartney   \n",
       "2  the watchdogs of china ruling communist party ...  JaneMacartney   \n",
       "\n",
       "                                           augmented  \n",
       "0  china has proceed step further from its recent...  \n",
       "1  the family of detained chinese dissident wang ...  \n",
       "2  the watchdogs of china find communist party ac...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccat50_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccat50_train['text'] = ccat50_train['text'].apply(preProcessText)\n",
    "ccat50_train['augmented'] = ccat50_train['augmented'].apply(preProcessText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "98GpsmYxsB1Q",
    "outputId": "89293a68-8242-45ed-c4c2-700042dded44"
   },
   "outputs": [],
   "source": [
    "trainx = pd.concat([ccat50_train['text'],ccat50_train['augmented']],ignore_index=True)\n",
    "trainy = pd.concat([ccat50_train['label'],ccat50_train['label']],ignore_index=True)\n",
    "# convert to dataframe\n",
    "trainx = pd.DataFrame(trainx,columns=['text'])\n",
    "trainy = pd.DataFrame(trainy,columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN7geXgmsB1a"
   },
   "outputs": [],
   "source": [
    "txt = ''\n",
    "docs = []\n",
    "sentences = []\n",
    "labels = []\n",
    "for cont, label in zip(trainx.text, trainy.label):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', clean(striphtml(cont)))\n",
    "    sentences = [sent.lower() for sent in sentences]\n",
    "    docs.append(sentences)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYGZ9_j4sB1d"
   },
   "outputs": [],
   "source": [
    "num_sent = []\n",
    "for doc in docs:\n",
    "    num_sent.append(len(doc))\n",
    "    for s in doc:\n",
    "        txt += s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HQyRz2YfsB1f",
    "outputId": "8c0c530f-417e-49b8-c496-1a69eec3b547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 29\n"
     ]
    }
   ],
   "source": [
    "chars = set(txt)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PpNvfBaNsB1j",
    "outputId": "09461fde-cbd5-417f-ab7a-fcde18485a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (len(docs)), print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rk8BcqnMsB1n",
    "outputId": "7102b516-8260-430a-ec0a-a43befbebea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing One hot encoding for training sample and targets:\n"
     ]
    }
   ],
   "source": [
    "print('Doing One hot encoding for training sample and targets:')\n",
    "x = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(labels)\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                x[i, j, (maxlen - 1 - t)] = char_indices[char]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Training X_train:[[23 16 21 ... 28 13  5]\n",
      " [-1 -1 -1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ... -1 -1 -1]\n",
      " ...\n",
      " [-1 -1 -1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ... -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "print('Sample Training X_train:{}'.format(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "aqdNOxxdsB1y",
    "outputId": "34aebd1a-638f-4f47-e491-88d8200140ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label: (5000, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds-nlp/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "# binary encode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "print('training label:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I5XrWTdrsB12",
    "outputId": "75b7a3d8-be6b-42ff-d6f7-3b94fb16711d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = []\n",
    "sent = []\n",
    "test_labels = []\n",
    "for cont, label in zip(ccat50_test.text, ccat50_test.label):\n",
    "    sent = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', \n",
    "                    clean(striphtml(cont)))\n",
    "    sent = [sent.lower() for sent in sent]\n",
    "    test_docs.append(sent)\n",
    "    test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing One hot encoding for testing sample and targets:\n",
      "(2500, 15, 512)\n"
     ]
    }
   ],
   "source": [
    "print('Doing One hot encoding for testing sample and targets:')\n",
    "x_test = np.ones((len(test_docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "print(x_test.shape)\n",
    "y_test = np.array(test_labels)\n",
    "for i, doc in enumerate(test_docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                #print(t)\n",
    "                x_test[i, j, (maxlen - 1 - t)] = char_indices[char]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label: (2500, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds-nlp/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_test)\n",
    "# binary encode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y_test = onehot_encoder.fit_transform(integer_encoded)\n",
    "print('training label:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPAQcFcVsB17"
   },
   "source": [
    "# Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_bZyf25VsB18"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKzuDTaSsB1-"
   },
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "   \n",
    "    def __init__(self, return_coefficients=False,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        \n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), a]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cfTw2STsB2B"
   },
   "source": [
    "# Precision, Recall and F-Score Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtVte8G0sB2B"
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkAUQa4nsB2I"
   },
   "source": [
    "# Tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hob-SISZsB2K"
   },
   "outputs": [],
   "source": [
    "# tensorbd_dir = 'drive/My Drive/Colab Notebooks/imbd62/'\n",
    "model_save_dir = '../../save_models/ccat50/'\n",
    "tensorboard = TensorBoard(log_dir=model_save_dir+'./ccat50_SGDW_logs',\n",
    "                         histogram_freq=0, \n",
    "                          write_graph=True, \n",
    "                         write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khu9u0qUsB2N"
   },
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCZH6SWQsB2O"
   },
   "outputs": [],
   "source": [
    "reduce_lr_adam = ReduceLROnPlateau(monitor='val_loss',factor=0.5,\n",
    "                                   patience=5,min_lr=1e-4)\n",
    "reduce_lr_sgd = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.5,patience=5, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earlystop and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                          patience=5,verbose=1,\n",
    "                                          mode='auto')\n",
    "num_epochs = 100\n",
    "#checkpointer\n",
    "checkpointer = ModelCheckpoint(model_save_dir+'ccat50_adamw_SGDW.hdf5', \n",
    "                               monitor='val_acc', \n",
    "                               verbose=1, save_best_only=True,\n",
    "                               mode='max')\n",
    "# CSV logger keras\n",
    "csv_logger = CSVLogger(model_save_dir+'ccat50_adamw_SGDW.csv', \n",
    "                       append=True, separator=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0nIu1KesB2X"
   },
   "source": [
    "# Convolutional layer with filter, windows and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W9FrJ7nrsB2Y"
   },
   "outputs": [],
   "source": [
    "# Convolutional layer parameter\n",
    "filter_length = [7, 3, 3]\n",
    "nb_filter = [512, 256,128]\n",
    "pool_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "bzcmQP92sB2a",
    "outputId": "8d5524a8-5c02-4848-a6c6-dfc1f894c392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentence input\n",
    "in_sentence = Input(shape=(maxlen,),\n",
    "                    dtype='int64',name='main_input1')\n",
    "# document input\n",
    "document = Input(shape=(max_sentences, maxlen), \n",
    "                 dtype='int64',name='main_input2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9VIea1rsB2c"
   },
   "outputs": [],
   "source": [
    "def binarize(x, sz=30):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "def binarize_outshape(in_shape):\n",
    "    return in_shape[0], in_shape[1], 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "PhwhF0VKsB2e",
    "outputId": "654c26e7-c703-40b1-c7a2-0399a8db6719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-4a1bbe77e1d2>:2: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "embedded = Lambda(binarize, output_shape=binarize_outshape, \n",
    "                  name='embed_input')(in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "from keras.initializers import glorot_normal, normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSQXhhRrsB2g"
   },
   "source": [
    "# Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "mFL-6sIKsB2g",
    "outputId": "306e2c83-db50-4da3-e335-bc7ba4f58368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input1 (InputLayer)     (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "embed_input (Lambda)         (None, 512, 30)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 506, 512)          108032    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 504, 256)          393472    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 502, 128)          98432     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 502, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 251, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 863,104\n",
      "Trainable params: 863,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedded: encodes sentence\n",
    "for i in range(len(nb_filter)):\n",
    "    embedded = Conv1D(filters=nb_filter[i],\n",
    "                      kernel_size=filter_length[i],\n",
    "                      padding='valid',\n",
    "                      activation='relu',\n",
    "                      kernel_initializer='glorot_normal',\n",
    "                      strides=1)(embedded)\n",
    "\n",
    "embedded = Dropout(0.3)(embedded)\n",
    "embedded = MaxPooling1D(pool_size=pool_length)(embedded)\n",
    "\n",
    "bi_lstm_sent = \\\n",
    "    Bidirectional(LSTM(128, return_sequences=False))(embedded)\n",
    "sent_encode = Dropout(0.3)(bi_lstm_sent)\n",
    "encoder = Model(inputs=in_sentence, outputs=sent_encode)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abxdg5iisB2k"
   },
   "source": [
    "# Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():    \n",
    "    encoded = TimeDistributed(encoder)(document)\n",
    "    # encoded: sentences to bi-lstm for document encoding \n",
    "    b_lstm_doc = \\\n",
    "        Bidirectional(LSTM(128, return_sequences=False))(encoded)\n",
    "#     output = AttentionWithContext()(b_lstm_doc)\n",
    "    output = Dropout(0.7)(b_lstm_doc)\n",
    "    output = Dense(1024, activation='relu')(output)\n",
    "    output = Dropout(0.5)(output)\n",
    "    output = Dense(nb_classes, activation='softmax')(output)\n",
    "    model = Model(inputs=document, outputs=output)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENGRcp2R3D4k"
   },
   "source": [
    "# Training with AdamW \n",
    "  * from: https://github.com/shaoanlu/AdamW-and-SGDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input2 (InputLayer)     (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 256)           863104    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                51250     \n",
      "=================================================================\n",
      "Total params: 1,571,762\n",
      "Trainable params: 1,571,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adam parameter\n",
    "model_ccat50 = build_cnn()\n",
    "b, B, T = batch_size, x.shape[0], num_epochs\n",
    "wd = 0.005 * (b/B/T)**0.5\n",
    "model_ccat50.compile(loss='categorical_crossentropy',\n",
    "                optimizer=AdamW(weight_decay=wd),metrics=['accuracy',f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_ccat50.save_weights(model_save_dir+\"saved_ccat50_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tq41WmlH3Avk",
    "outputId": "025d9882-32a7-453d-ad82-0f44e947e6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Processing fold # 0\n",
      "-----------------------------\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "4000/4000 [==============================] - 296s 74ms/step - loss: 3.8880 - acc: 0.0227 - f1: 0.0000e+00 - val_loss: 4.1968 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 2/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 3.7744 - acc: 0.0398 - f1: 4.8485e-04 - val_loss: 4.1386 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "Epoch 3/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 3.4722 - acc: 0.0877 - f1: 0.0456 - val_loss: 3.9196 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "Epoch 4/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 3.1774 - acc: 0.1305 - f1: 0.0889 - val_loss: 3.7817 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "Epoch 5/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 2.9308 - acc: 0.1668 - f1: 0.1250 - val_loss: 3.4226 - val_acc: 0.0410 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.00000 to 0.04100, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 6/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 2.7106 - acc: 0.1903 - f1: 0.1405 - val_loss: 3.1980 - val_acc: 0.0790 - val_f1: 0.0456\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.04100 to 0.07900, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 7/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 2.4819 - acc: 0.2415 - f1: 0.1768 - val_loss: 2.9086 - val_acc: 0.1140 - val_f1: 0.0517\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.07900 to 0.11400, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 8/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 2.2273 - acc: 0.2925 - f1: 0.2228 - val_loss: 2.7849 - val_acc: 0.1220 - val_f1: 0.0798\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.11400 to 0.12200, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 9/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 2.0643 - acc: 0.3285 - f1: 0.2465 - val_loss: 2.6911 - val_acc: 0.1620 - val_f1: 0.0798\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.12200 to 0.16200, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 10/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 1.8789 - acc: 0.3747 - f1: 0.2966 - val_loss: 2.3945 - val_acc: 0.2120 - val_f1: 0.1154\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.16200 to 0.21200, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 11/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 1.7272 - acc: 0.4153 - f1: 0.3301 - val_loss: 2.2930 - val_acc: 0.2280 - val_f1: 0.1321\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.21200 to 0.22800, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 12/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 1.6041 - acc: 0.4425 - f1: 0.3612 - val_loss: 2.0526 - val_acc: 0.2810 - val_f1: 0.1642\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.22800 to 0.28100, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 13/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 1.4942 - acc: 0.4860 - f1: 0.4222 - val_loss: 2.0952 - val_acc: 0.2960 - val_f1: 0.1742\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.28100 to 0.29600, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 14/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 1.4090 - acc: 0.5037 - f1: 0.4563 - val_loss: 2.2014 - val_acc: 0.2520 - val_f1: 0.1572\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.29600\n",
      "Epoch 15/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 1.3272 - acc: 0.5473 - f1: 0.5052 - val_loss: 1.9306 - val_acc: 0.3180 - val_f1: 0.1916\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.29600 to 0.31800, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 16/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 1.2030 - acc: 0.5820 - f1: 0.5452 - val_loss: 1.6941 - val_acc: 0.3970 - val_f1: 0.2291\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.31800 to 0.39700, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 17/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 1.1260 - acc: 0.6160 - f1: 0.5919 - val_loss: 1.6473 - val_acc: 0.4210 - val_f1: 0.2818\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.39700 to 0.42100, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 18/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.9969 - acc: 0.6540 - f1: 0.6286 - val_loss: 1.4784 - val_acc: 0.4900 - val_f1: 0.2807\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.42100 to 0.49000, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 19/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.9228 - acc: 0.6853 - f1: 0.6599 - val_loss: 1.5067 - val_acc: 0.4630 - val_f1: 0.3356\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.49000\n",
      "Epoch 20/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.8644 - acc: 0.7060 - f1: 0.6896 - val_loss: 1.4328 - val_acc: 0.5010 - val_f1: 0.4025\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.49000 to 0.50100, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 21/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.8068 - acc: 0.7210 - f1: 0.7134 - val_loss: 1.3948 - val_acc: 0.5670 - val_f1: 0.4540\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.50100 to 0.56700, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 22/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.7237 - acc: 0.7548 - f1: 0.7450 - val_loss: 1.1146 - val_acc: 0.6500 - val_f1: 0.5964\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.56700 to 0.65000, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 23/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.6828 - acc: 0.7702 - f1: 0.7615 - val_loss: 1.4124 - val_acc: 0.5750 - val_f1: 0.5183\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.65000\n",
      "Epoch 24/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.6014 - acc: 0.7963 - f1: 0.7910 - val_loss: 1.1062 - val_acc: 0.6760 - val_f1: 0.6532\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.65000 to 0.67600, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 25/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.5932 - acc: 0.8083 - f1: 0.8014 - val_loss: 0.9878 - val_acc: 0.6980 - val_f1: 0.6715\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.67600 to 0.69800, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 26/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.5178 - acc: 0.8250 - f1: 0.8228 - val_loss: 0.9605 - val_acc: 0.7140 - val_f1: 0.7089\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.69800 to 0.71400, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 27/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.4756 - acc: 0.8413 - f1: 0.8422 - val_loss: 0.9637 - val_acc: 0.7280 - val_f1: 0.7152\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.71400 to 0.72800, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 28/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.4060 - acc: 0.8638 - f1: 0.8600 - val_loss: 1.0216 - val_acc: 0.7200 - val_f1: 0.7055\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.72800\n",
      "Epoch 29/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.4242 - acc: 0.8618 - f1: 0.8636 - val_loss: 1.0257 - val_acc: 0.7200 - val_f1: 0.7116\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.72800\n",
      "Epoch 30/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.3626 - acc: 0.8825 - f1: 0.8835 - val_loss: 0.9928 - val_acc: 0.7310 - val_f1: 0.7184\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.72800 to 0.73100, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 31/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.4124 - acc: 0.8755 - f1: 0.8733 - val_loss: 0.9046 - val_acc: 0.7620 - val_f1: 0.7536\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.73100 to 0.76200, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 32/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.3527 - acc: 0.8872 - f1: 0.8872 - val_loss: 0.9458 - val_acc: 0.7550 - val_f1: 0.7454\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76200\n",
      "Epoch 33/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.3166 - acc: 0.9020 - f1: 0.9044 - val_loss: 0.9842 - val_acc: 0.7370 - val_f1: 0.7353\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76200\n",
      "Epoch 34/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.2908 - acc: 0.9075 - f1: 0.9098 - val_loss: 0.9319 - val_acc: 0.7720 - val_f1: 0.7731\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.76200 to 0.77200, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 35/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.3310 - acc: 0.8988 - f1: 0.9001 - val_loss: 0.9062 - val_acc: 0.7750 - val_f1: 0.7715\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.77200 to 0.77500, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 36/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.2758 - acc: 0.9180 - f1: 0.9176 - val_loss: 1.0533 - val_acc: 0.7740 - val_f1: 0.7750\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.77500\n",
      "Epoch 00036: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 1\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.4216 - acc: 0.8772 - f1: 0.8777 - val_loss: 0.0572 - val_acc: 0.9870 - val_f1: 0.9879\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.77500 to 0.98700, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 2/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.2338 - acc: 0.9257 - f1: 0.9251 - val_loss: 0.0366 - val_acc: 0.9860 - val_f1: 0.9884\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.98700\n",
      "Epoch 3/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1945 - acc: 0.9425 - f1: 0.9417 - val_loss: 0.0423 - val_acc: 0.9880 - val_f1: 0.9880\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98700 to 0.98800, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 4/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.1517 - acc: 0.9530 - f1: 0.9532 - val_loss: 0.0449 - val_acc: 0.9870 - val_f1: 0.9884\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.98800\n",
      "Epoch 5/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1516 - acc: 0.9560 - f1: 0.9547 - val_loss: 0.0247 - val_acc: 0.9930 - val_f1: 0.9934\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98800 to 0.99300, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 6/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1419 - acc: 0.9613 - f1: 0.9612 - val_loss: 0.0263 - val_acc: 0.9930 - val_f1: 0.9939\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99300\n",
      "Epoch 7/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.1201 - acc: 0.9620 - f1: 0.9624 - val_loss: 0.0225 - val_acc: 0.9950 - val_f1: 0.9950\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.99300 to 0.99500, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 8/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1104 - acc: 0.9667 - f1: 0.9660 - val_loss: 0.0424 - val_acc: 0.9860 - val_f1: 0.9863\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99500\n",
      "Epoch 9/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1045 - acc: 0.9673 - f1: 0.9676 - val_loss: 0.0289 - val_acc: 0.9940 - val_f1: 0.9930\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99500\n",
      "Epoch 10/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1148 - acc: 0.9645 - f1: 0.9653 - val_loss: 0.0327 - val_acc: 0.9870 - val_f1: 0.9869\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99500\n",
      "Epoch 11/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.1054 - acc: 0.9670 - f1: 0.9673 - val_loss: 0.0476 - val_acc: 0.9860 - val_f1: 0.9869\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99500\n",
      "Epoch 12/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.1017 - acc: 0.9695 - f1: 0.9696 - val_loss: 0.0408 - val_acc: 0.9860 - val_f1: 0.9850\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99500\n",
      "Epoch 00012: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 2\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.0981 - acc: 0.9688 - f1: 0.9700 - val_loss: 0.0108 - val_acc: 0.9970 - val_f1: 0.9970\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.99500 to 0.99700, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 2/80\n",
      "4000/4000 [==============================] - 291s 73ms/step - loss: 0.0552 - acc: 0.9850 - f1: 0.9848 - val_loss: 0.0141 - val_acc: 0.9930 - val_f1: 0.9930\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99700\n",
      "Epoch 3/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0670 - acc: 0.9802 - f1: 0.9799 - val_loss: 0.0100 - val_acc: 0.9950 - val_f1: 0.9955\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99700\n",
      "Epoch 4/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0522 - acc: 0.9848 - f1: 0.9845 - val_loss: 0.0107 - val_acc: 0.9950 - val_f1: 0.9955\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99700\n",
      "Epoch 5/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0557 - acc: 0.9848 - f1: 0.9846 - val_loss: 0.0045 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.99700 to 0.99900, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 6/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0592 - acc: 0.9815 - f1: 0.9822 - val_loss: 0.0074 - val_acc: 0.9970 - val_f1: 0.9965\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99900\n",
      "Epoch 7/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0408 - acc: 0.9878 - f1: 0.9873 - val_loss: 0.0068 - val_acc: 0.9960 - val_f1: 0.9960\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99900\n",
      "Epoch 8/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0405 - acc: 0.9880 - f1: 0.9880 - val_loss: 0.0054 - val_acc: 0.9980 - val_f1: 0.9980\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99900\n",
      "Epoch 9/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0398 - acc: 0.9885 - f1: 0.9890 - val_loss: 0.0087 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99900\n",
      "Epoch 10/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0429 - acc: 0.9882 - f1: 0.9878 - val_loss: 0.0047 - val_acc: 0.9990 - val_f1: 0.9995\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99900\n",
      "Epoch 00010: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 3\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0412 - acc: 0.9878 - f1: 0.9882 - val_loss: 0.0029 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.99900\n",
      "Epoch 2/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0384 - acc: 0.9890 - f1: 0.9887 - val_loss: 0.0030 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99900\n",
      "Epoch 3/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0345 - acc: 0.9900 - f1: 0.9898 - val_loss: 0.0029 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99900\n",
      "Epoch 4/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0342 - acc: 0.9885 - f1: 0.9882 - val_loss: 0.0017 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99900\n",
      "Epoch 5/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0300 - acc: 0.9920 - f1: 0.9921 - val_loss: 0.0016 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99900\n",
      "Epoch 6/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0294 - acc: 0.9903 - f1: 0.9907 - val_loss: 0.0036 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99900\n",
      "Epoch 7/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0231 - acc: 0.9942 - f1: 0.9945 - val_loss: 0.0044 - val_acc: 0.9980 - val_f1: 0.9980\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99900\n",
      "Epoch 8/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0240 - acc: 0.9928 - f1: 0.9931 - val_loss: 0.0039 - val_acc: 0.9990 - val_f1: 0.9990\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99900\n",
      "Epoch 9/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0237 - acc: 0.9942 - f1: 0.9941 - val_loss: 0.0041 - val_acc: 0.9990 - val_f1: 0.9985\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99900\n",
      "Epoch 10/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0259 - acc: 0.9925 - f1: 0.9924 - val_loss: 0.0062 - val_acc: 0.9980 - val_f1: 0.9980\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99900\n",
      "Epoch 00010: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 4\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0274 - acc: 0.9905 - f1: 0.9905 - val_loss: 2.5765e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.99900 to 1.00000, saving model to ../../save_models/ccat50/ccat50_adamw.hdf5\n",
      "Epoch 2/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0204 - acc: 0.9942 - f1: 0.9945 - val_loss: 1.8516e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Epoch 3/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0223 - acc: 0.9922 - f1: 0.9925 - val_loss: 2.3982e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "Epoch 4/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0216 - acc: 0.9945 - f1: 0.9944 - val_loss: 2.4289e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "Epoch 5/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0255 - acc: 0.9930 - f1: 0.9929 - val_loss: 1.5843e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 1.00000\n",
      "Epoch 6/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0181 - acc: 0.9952 - f1: 0.9951 - val_loss: 1.7743e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 1.00000\n",
      "Epoch 7/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0262 - acc: 0.9910 - f1: 0.9907 - val_loss: 2.1102e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 1.00000\n",
      "Epoch 8/80\n",
      "4000/4000 [==============================] - 290s 73ms/step - loss: 0.0207 - acc: 0.9922 - f1: 0.9926 - val_loss: 2.6551e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 1.00000\n",
      "Epoch 9/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0208 - acc: 0.9935 - f1: 0.9934 - val_loss: 2.7429e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 1.00000\n",
      "Epoch 10/80\n",
      "4000/4000 [==============================] - 290s 72ms/step - loss: 0.0222 - acc: 0.9940 - f1: 0.9937 - val_loss: 2.0835e-05 - val_acc: 1.0000 - val_f1: 1.0000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 1.00000\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "scores = []\n",
    "num_validation_sample = len(x)//k\n",
    "for i in range(k):\n",
    "    print(\"-----------------------------\")\n",
    "    print('Processing fold #', i)\n",
    "    print(\"-----------------------------\")\n",
    "    val_data = x[i * num_validation_sample: (i + 1) * num_validation_sample]\n",
    "    val_lab  = y[i * num_validation_sample: (i + 1) * num_validation_sample]\n",
    "    \n",
    "    parttial_train_X_data = np.concatenate(\n",
    "                [x[:i * num_validation_sample],\n",
    "                x[(i + 1) * num_validation_sample:]], axis=0)\n",
    "    \n",
    "    parttial_train_X_label = np.concatenate(\n",
    "                [y[:i * num_validation_sample],\n",
    "                y[(i + 1) * num_validation_sample:]], axis=0)\n",
    "    \n",
    "    history_ccat50_atten = model_ccat50.fit(parttial_train_X_data, parttial_train_X_label, \n",
    "                               validation_data=(val_data,val_lab),\n",
    "                               batch_size=batch_size,epochs=num_epochs,verbose=1,\n",
    "                               callbacks=[reduce_lr_adam,earlystop,csv_logger,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jo2bQBcZ9YV2",
    "outputId": "824f424c-5b3f-4376-a91c-efebb7f6c0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9930499999999999\n",
      "------------\n",
      "1.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "#Print average acc\n",
    "average_acc = np.mean(history_ccat50_atten.history['acc'])\n",
    "print(average_acc)\n",
    "print(\"------------\")\n",
    "#Print average val_acc\n",
    "average_val_acc = np.mean(history_ccat50_atten.history['val_acc'])\n",
    "print(average_val_acc)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "KNfYwFVNlpDQ",
    "outputId": "3eae5383-722a-46e2-f6b8-1ae9c76d90a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0225146318156505\n",
      "------------\n",
      "2.2205487702967734e-05\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "#Print average loss\n",
    "average_loss = np.mean(history_ccat50_atten.history['loss'])\n",
    "print(average_loss)\n",
    "print(\"------------\")\n",
    "#Print average val_loss\n",
    "average_val_loss = np.mean(history_ccat50_atten.history['val_loss'])\n",
    "print(average_val_loss)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xlLgDrzHlsuS",
    "outputId": "0e28ccba-7787-457c-c39f-8270d6f57b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99302419257164\n",
      "------------\n",
      "1.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "#Print average f1-score\n",
    "average_f1 = np.mean(history_ccat50_atten.history['f1'])\n",
    "print(average_f1)\n",
    "print(\"------------\")\n",
    "#Print average val_f1-score\n",
    "average_val_f1 = np.mean(history_ccat50_atten.history['val_f1'])\n",
    "print(average_val_f1)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "bixPGBEllvEH",
    "outputId": "47a1f55a-3ace-4282-98d0-23760e60b174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 19s 19ms/step\n",
      "Test loss: 2.0834818082676065e-05\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model on the 20% Validation Dataset\n",
    "score =  model_ccat50.evaluate(val_data, val_lab, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAW8NCg_lxgX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 48s 19ms/step\n",
      "Test loss:  10.779514399719238\n",
      "Test accuracy: 0.194\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model on the 20% Test Dataset\n",
    "scorev =  model_ccat50.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss: ', scorev[0])\n",
    "print('Test accuracy:', scorev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "es_m-1HrM_Ce"
   },
   "outputs": [],
   "source": [
    "prediction_value = model_ccat50.predict(x_test)\n",
    "predict_class = np.argmax(prediction_value, axis=-1)\n",
    "y_test_integer = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.10      0.10        50\n",
      "           1       0.33      0.30      0.31        50\n",
      "           2       0.10      0.04      0.06        50\n",
      "           3       0.04      0.12      0.06        50\n",
      "           4       0.41      0.30      0.34        50\n",
      "           5       0.03      0.06      0.04        50\n",
      "           6       0.06      0.08      0.07        50\n",
      "           7       0.11      0.06      0.08        50\n",
      "           8       0.09      0.04      0.06        50\n",
      "           9       0.67      0.24      0.35        50\n",
      "          10       0.15      0.20      0.17        50\n",
      "          11       0.04      0.10      0.06        50\n",
      "          12       0.07      0.14      0.09        50\n",
      "          13       0.24      0.18      0.21        50\n",
      "          14       0.02      0.02      0.02        50\n",
      "          15       1.00      0.94      0.97        50\n",
      "          16       0.43      0.18      0.25        50\n",
      "          17       0.03      0.02      0.02        50\n",
      "          18       0.13      0.14      0.13        50\n",
      "          19       0.14      0.26      0.18        50\n",
      "          20       0.35      0.12      0.18        50\n",
      "          21       0.09      0.02      0.03        50\n",
      "          22       0.43      0.46      0.45        50\n",
      "          23       0.41      0.18      0.25        50\n",
      "          24       0.00      0.00      0.00        50\n",
      "          25       0.05      0.02      0.03        50\n",
      "          26       0.12      0.04      0.06        50\n",
      "          27       0.21      0.08      0.12        50\n",
      "          28       0.53      0.50      0.52        50\n",
      "          29       0.31      0.38      0.34        50\n",
      "          30       0.43      0.46      0.45        50\n",
      "          31       0.89      0.16      0.27        50\n",
      "          32       0.92      0.72      0.81        50\n",
      "          33       0.75      0.36      0.49        50\n",
      "          34       0.08      0.04      0.05        50\n",
      "          35       0.89      0.48      0.62        50\n",
      "          36       0.42      0.26      0.32        50\n",
      "          37       0.07      0.06      0.06        50\n",
      "          38       0.27      0.18      0.22        50\n",
      "          39       0.06      0.06      0.06        50\n",
      "          40       0.14      0.04      0.06        50\n",
      "          41       0.83      0.30      0.44        50\n",
      "          42       0.10      0.06      0.07        50\n",
      "          43       0.05      0.04      0.04        50\n",
      "          44       0.28      0.66      0.39        50\n",
      "          45       0.05      0.10      0.07        50\n",
      "          46       0.04      0.12      0.06        50\n",
      "          47       0.19      0.06      0.09        50\n",
      "          48       0.27      0.06      0.10        50\n",
      "          49       0.04      0.16      0.06        50\n",
      "\n",
      "    accuracy                           0.19      2500\n",
      "   macro avg       0.27      0.19      0.21      2500\n",
      "weighted avg       0.27      0.19      0.21      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test_integer, predict_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGUEP_TRVAwo"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsource = '../../save_models/ccat50/'\n",
    "df_ccat50 = pd.read_csv(modelsource +'/ccat50_adamw.csv',engine='python',sep=';')\n",
    "# df_movies.to_excel(modelsource+'/imbd622.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.904880</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.145324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.891559</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.090615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.884626</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.175266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.877156</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.202045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.02350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.872223</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.239463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.903327</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.083933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.885838</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.101827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.879899</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.166552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.873158</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.250594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.890564</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.262088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.874007</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.300010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.873098</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.302232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.871580</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.244122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.02200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.871344</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.278237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>0.02075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.871676</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>0.02525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.870715</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.285547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>0.02075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.871508</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.354226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>0.02550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.869984</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.314060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.937429</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.962212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.892297</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.100429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.04950</td>\n",
       "      <td>0.034080</td>\n",
       "      <td>3.728809</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.183580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.887992</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.196799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.03975</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>3.774417</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.138596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.08775</td>\n",
       "      <td>0.045589</td>\n",
       "      <td>3.472239</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.919560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>0.13050</td>\n",
       "      <td>0.088949</td>\n",
       "      <td>3.177447</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.781722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>0.16675</td>\n",
       "      <td>0.125030</td>\n",
       "      <td>2.930781</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.422559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>0.19025</td>\n",
       "      <td>0.140491</td>\n",
       "      <td>2.710646</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.045645</td>\n",
       "      <td>3.198032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>0.24150</td>\n",
       "      <td>0.176785</td>\n",
       "      <td>2.481892</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.051674</td>\n",
       "      <td>2.908595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7</td>\n",
       "      <td>0.29250</td>\n",
       "      <td>0.222834</td>\n",
       "      <td>2.227333</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.079837</td>\n",
       "      <td>2.784946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>0.32850</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>2.064337</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.079837</td>\n",
       "      <td>2.691080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>0.969960</td>\n",
       "      <td>0.098106</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.010757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>0.98500</td>\n",
       "      <td>0.984798</td>\n",
       "      <td>0.055193</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.992968</td>\n",
       "      <td>0.014137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>0.98025</td>\n",
       "      <td>0.979928</td>\n",
       "      <td>0.067009</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995492</td>\n",
       "      <td>0.010002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3</td>\n",
       "      <td>0.98475</td>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.995492</td>\n",
       "      <td>0.010718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4</td>\n",
       "      <td>0.98475</td>\n",
       "      <td>0.984575</td>\n",
       "      <td>0.055681</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5</td>\n",
       "      <td>0.98150</td>\n",
       "      <td>0.982198</td>\n",
       "      <td>0.059222</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.996492</td>\n",
       "      <td>0.007366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6</td>\n",
       "      <td>0.98775</td>\n",
       "      <td>0.987337</td>\n",
       "      <td>0.040832</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.006760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7</td>\n",
       "      <td>0.98800</td>\n",
       "      <td>0.987976</td>\n",
       "      <td>0.040455</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>8</td>\n",
       "      <td>0.98850</td>\n",
       "      <td>0.988964</td>\n",
       "      <td>0.039757</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.008663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>9</td>\n",
       "      <td>0.98825</td>\n",
       "      <td>0.987825</td>\n",
       "      <td>0.042915</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>0.004657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0.98775</td>\n",
       "      <td>0.988222</td>\n",
       "      <td>0.041155</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>0.98900</td>\n",
       "      <td>0.988710</td>\n",
       "      <td>0.038412</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.003011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>0.989841</td>\n",
       "      <td>0.034457</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.002884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3</td>\n",
       "      <td>0.98850</td>\n",
       "      <td>0.988222</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4</td>\n",
       "      <td>0.99200</td>\n",
       "      <td>0.992119</td>\n",
       "      <td>0.030022</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.001630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5</td>\n",
       "      <td>0.99025</td>\n",
       "      <td>0.990726</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.003624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6</td>\n",
       "      <td>0.99425</td>\n",
       "      <td>0.994484</td>\n",
       "      <td>0.023118</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.004397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7</td>\n",
       "      <td>0.99275</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.024047</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.003933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>8</td>\n",
       "      <td>0.99425</td>\n",
       "      <td>0.994115</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.998492</td>\n",
       "      <td>0.004116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>9</td>\n",
       "      <td>0.99250</td>\n",
       "      <td>0.992365</td>\n",
       "      <td>0.025866</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.006185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99050</td>\n",
       "      <td>0.990484</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>0.99425</td>\n",
       "      <td>0.994492</td>\n",
       "      <td>0.020420</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2</td>\n",
       "      <td>0.99225</td>\n",
       "      <td>0.992496</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3</td>\n",
       "      <td>0.99450</td>\n",
       "      <td>0.994365</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>0.992865</td>\n",
       "      <td>0.025470</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5</td>\n",
       "      <td>0.99525</td>\n",
       "      <td>0.995111</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6</td>\n",
       "      <td>0.99100</td>\n",
       "      <td>0.990730</td>\n",
       "      <td>0.026196</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>7</td>\n",
       "      <td>0.99225</td>\n",
       "      <td>0.992607</td>\n",
       "      <td>0.020683</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8</td>\n",
       "      <td>0.99350</td>\n",
       "      <td>0.993353</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>0.993738</td>\n",
       "      <td>0.022219</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      acc        f1      loss        lr  val_acc    val_f1  val_loss\n",
       "0       0  0.02575  0.000000  3.904880  0.001000    0.000  0.000000  4.145324\n",
       "1       1  0.01900  0.000000  3.891559  0.001000    0.000  0.000000  4.090615\n",
       "2       2  0.02175  0.000000  3.884626  0.001000    0.000  0.000000  4.175266\n",
       "3       3  0.02250  0.000000  3.877156  0.001000    0.000  0.000000  4.202045\n",
       "4       4  0.02350  0.000000  3.872223  0.001000    0.000  0.000000  4.239463\n",
       "5       0  0.02300  0.000000  3.903327  0.001000    0.000  0.000000  4.083933\n",
       "6       1  0.02525  0.000000  3.885838  0.001000    0.000  0.000000  4.101827\n",
       "7       2  0.02350  0.000000  3.879899  0.001000    0.000  0.000000  4.166552\n",
       "8       3  0.02175  0.000000  3.873158  0.001000    0.000  0.000000  4.250594\n",
       "9       0  0.02175  0.000000  3.890564  0.001000    0.000  0.000000  4.262088\n",
       "10      1  0.02225  0.000000  3.874007  0.001000    0.000  0.000000  4.300010\n",
       "11      2  0.02250  0.000000  3.873098  0.001000    0.000  0.000000  4.302232\n",
       "12      3  0.02150  0.000000  3.871580  0.001000    0.000  0.000000  4.244122\n",
       "13      4  0.02200  0.000000  3.871344  0.001000    0.000  0.000000  4.278237\n",
       "14      5  0.02075  0.000000  3.871676  0.001000    0.000  0.000000  4.289474\n",
       "15      6  0.02525  0.000000  3.870715  0.001000    0.000  0.000000  4.285547\n",
       "16      7  0.02075  0.000000  3.871508  0.001000    0.000  0.000000  4.354226\n",
       "17      8  0.02550  0.000000  3.869984  0.001000    0.000  0.000000  4.314060\n",
       "18      0  0.02175  0.000000  3.937429  0.000500    0.000  0.000000  3.962212\n",
       "19      0  0.02025  0.000000  3.892297  0.001000    0.000  0.000000  4.100429\n",
       "20      1  0.04950  0.034080  3.728809  0.001000    0.000  0.000000  4.183580\n",
       "21      0  0.02275  0.000000  3.887992  0.001000    0.000  0.000000  4.196799\n",
       "22      1  0.03975  0.000485  3.774417  0.001000    0.000  0.000000  4.138596\n",
       "23      2  0.08775  0.045589  3.472239  0.001000    0.000  0.000000  3.919560\n",
       "24      3  0.13050  0.088949  3.177447  0.001000    0.000  0.000000  3.781722\n",
       "25      4  0.16675  0.125030  2.930781  0.001000    0.041  0.000000  3.422559\n",
       "26      5  0.19025  0.140491  2.710646  0.001000    0.079  0.045645  3.198032\n",
       "27      6  0.24150  0.176785  2.481892  0.001000    0.114  0.051674  2.908595\n",
       "28      7  0.29250  0.222834  2.227333  0.001000    0.122  0.079837  2.784946\n",
       "29      8  0.32850  0.246493  2.064337  0.001000    0.162  0.079837  2.691080\n",
       "..    ...      ...       ...       ...       ...      ...       ...       ...\n",
       "69      0  0.96875  0.969960  0.098106  0.000250    0.997  0.997000  0.010757\n",
       "70      1  0.98500  0.984798  0.055193  0.000250    0.993  0.992968  0.014137\n",
       "71      2  0.98025  0.979928  0.067009  0.000250    0.995  0.995492  0.010002\n",
       "72      3  0.98475  0.984472  0.052200  0.000250    0.995  0.995492  0.010718\n",
       "73      4  0.98475  0.984575  0.055681  0.000250    0.999  0.999000  0.004511\n",
       "74      5  0.98150  0.982198  0.059222  0.000250    0.997  0.996492  0.007366\n",
       "75      6  0.98775  0.987337  0.040832  0.000250    0.996  0.996000  0.006760\n",
       "76      7  0.98800  0.987976  0.040455  0.000250    0.998  0.998000  0.005405\n",
       "77      8  0.98850  0.988964  0.039757  0.000250    0.999  0.999000  0.008663\n",
       "78      9  0.98825  0.987825  0.042915  0.000250    0.999  0.999492  0.004657\n",
       "79      0  0.98775  0.988222  0.041155  0.000125    0.999  0.999000  0.002900\n",
       "80      1  0.98900  0.988710  0.038412  0.000125    0.999  0.999000  0.003011\n",
       "81      2  0.99000  0.989841  0.034457  0.000125    0.999  0.999000  0.002884\n",
       "82      3  0.98850  0.988222  0.034176  0.000125    0.999  0.999000  0.001668\n",
       "83      4  0.99200  0.992119  0.030022  0.000125    0.999  0.999000  0.001630\n",
       "84      5  0.99025  0.990726  0.029392  0.000125    0.999  0.999000  0.003624\n",
       "85      6  0.99425  0.994484  0.023118  0.000125    0.998  0.998000  0.004397\n",
       "86      7  0.99275  0.993103  0.024047  0.000125    0.999  0.999000  0.003933\n",
       "87      8  0.99425  0.994115  0.023733  0.000125    0.999  0.998492  0.004116\n",
       "88      9  0.99250  0.992365  0.025866  0.000100    0.998  0.998000  0.006185\n",
       "89      0  0.99050  0.990484  0.027371  0.000100    1.000  1.000000  0.000026\n",
       "90      1  0.99425  0.994492  0.020420  0.000100    1.000  1.000000  0.000019\n",
       "91      2  0.99225  0.992496  0.022291  0.000100    1.000  1.000000  0.000024\n",
       "92      3  0.99450  0.994365  0.021623  0.000100    1.000  1.000000  0.000024\n",
       "93      4  0.99300  0.992865  0.025470  0.000100    1.000  1.000000  0.000016\n",
       "94      5  0.99525  0.995111  0.018079  0.000100    1.000  1.000000  0.000018\n",
       "95      6  0.99100  0.990730  0.026196  0.000100    1.000  1.000000  0.000021\n",
       "96      7  0.99225  0.992607  0.020683  0.000100    1.000  1.000000  0.000027\n",
       "97      8  0.99350  0.993353  0.020793  0.000100    1.000  1.000000  0.000027\n",
       "98      9  0.99400  0.993738  0.022219  0.000100    1.000  1.000000  0.000021\n",
       "\n",
       "[99 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ccat50.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2144658668>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ccat50.plot('epoch','lr',kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training accuracy and validate 4 CCAT50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'AT-BLSTM model accuracy for CCAT50')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_ccat50.index, df_ccat50['acc'], 'go-', color='red', label= 'acc',linewidth=3)\n",
    "plt.plot(df_ccat50.index, df_ccat50['val_acc'], '--^', color='blue',label= 'val_acc',linewidth=3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuarcy')\n",
    "plt.savefig(modelsource + 'ccat50_acc.png')\n",
    "plt.title('AT-BLSTM model accuracy for CCAT50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loss and validate 4 CCAT50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'AT-BLSTM model loss for CCAT50')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_ccat50.index, df_ccat50['loss'], 'go-', color='red', label= 'loss',linewidth=3)\n",
    "plt.plot(df_ccat50.index, df_ccat50['val_loss'], '--^', color='blue',label= 'val_loss',linewidth=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(modelsource + 'ccat50_loss.png')\n",
    "plt.title('AT-BLSTM model loss for CCAT50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'AT-BLSTM model F-score for CCAT50')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_ccat50.index, df_ccat50['f1'], 'go-', color='red', label= 'f1',linewidth=3)\n",
    "plt.plot(df_ccat50.index, df_ccat50['val_f1'], '--*', color='blue',label= 'val_f1',linewidth=3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-score')\n",
    "plt.savefig(modelsource + 'ccat50_F1-score.png')\n",
    "plt.title('AT-BLSTM model F-score for CCAT50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SGDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input2 (InputLayer)     (None, 15, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 15, 256)           863104    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                51250     \n",
      "=================================================================\n",
      "Total params: 1,571,762\n",
      "Trainable params: 1,571,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "model_SGDW = build_cnn()\n",
    "b, B, T = batch_size, x.shape[0], num_epochs\n",
    "wd = 0.0025 * (b/B/T)**0.5\n",
    "model_SGDW.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=SGDW(weight_decay=wd, momentum=0.9),\n",
    "                 metrics=['accuracy',f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_SGDW.save_weights(model_save_dir+\"saved_IMBD62_model_SGDW.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Processing fold # 0\n",
      "-----------------------------\n",
      "WARNING:tensorflow:From /home/ds-nlp/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 286s 71ms/step - loss: 3.9096 - acc: 0.0208 - f1: 0.0000e+00 - val_loss: 3.9450 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to ../../save_models/ccat50/ccat50_adamw_SGDW.hdf5\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 280s 70ms/step - loss: 3.9004 - acc: 0.0263 - f1: 0.0000e+00 - val_loss: 3.9936 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.8927 - acc: 0.0238 - f1: 0.0000e+00 - val_loss: 4.0402 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.8854 - acc: 0.0243 - f1: 0.0000e+00 - val_loss: 4.0865 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.8794 - acc: 0.0248 - f1: 0.0000e+00 - val_loss: 4.1335 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.8759 - acc: 0.0225 - f1: 0.0000e+00 - val_loss: 4.1763 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "Epoch 00006: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 1\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9666 - acc: 0.0185 - f1: 0.0000e+00 - val_loss: 3.8072 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.00000\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9467 - acc: 0.0223 - f1: 0.0000e+00 - val_loss: 3.8484 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9321 - acc: 0.0250 - f1: 0.0000e+00 - val_loss: 3.8824 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9219 - acc: 0.0220 - f1: 0.0000e+00 - val_loss: 3.9121 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9140 - acc: 0.0243 - f1: 0.0000e+00 - val_loss: 3.9398 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9073 - acc: 0.0267 - f1: 0.0000e+00 - val_loss: 3.9658 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "Epoch 00006: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 2\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9290 - acc: 0.0123 - f1: 0.0000e+00 - val_loss: 3.8630 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.00000 to 0.05000, saving model to ../../save_models/ccat50/ccat50_adamw_SGDW.hdf5\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9263 - acc: 0.0132 - f1: 0.0000e+00 - val_loss: 3.8783 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.05000\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9211 - acc: 0.0150 - f1: 0.0000e+00 - val_loss: 3.8930 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.05000\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 278s 69ms/step - loss: 3.9174 - acc: 0.0130 - f1: 0.0000e+00 - val_loss: 3.9070 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.05000\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9133 - acc: 0.0160 - f1: 0.0000e+00 - val_loss: 3.9196 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.05000\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 278s 69ms/step - loss: 3.9117 - acc: 0.0108 - f1: 0.0000e+00 - val_loss: 3.9313 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.05000\n",
      "Epoch 00006: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 3\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 278s 69ms/step - loss: 3.9185 - acc: 0.0255 - f1: 0.0000e+00 - val_loss: 3.8990 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.05000\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 278s 69ms/step - loss: 3.9156 - acc: 0.0217 - f1: 0.0000e+00 - val_loss: 3.9060 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.05000\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9143 - acc: 0.0220 - f1: 0.0000e+00 - val_loss: 3.9126 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.05000\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9125 - acc: 0.0225 - f1: 0.0000e+00 - val_loss: 3.9195 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.05000\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9110 - acc: 0.0170 - f1: 0.0000e+00 - val_loss: 3.9263 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.05000\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9090 - acc: 0.0248 - f1: 0.0000e+00 - val_loss: 3.9325 - val_acc: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.05000\n",
      "Epoch 00006: early stopping\n",
      "-----------------------------\n",
      "Processing fold # 4\n",
      "-----------------------------\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9251 - acc: 0.0135 - f1: 0.0000e+00 - val_loss: 3.8675 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.05000\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9242 - acc: 0.0097 - f1: 0.0000e+00 - val_loss: 3.8713 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.05000\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9237 - acc: 0.0125 - f1: 0.0000e+00 - val_loss: 3.8750 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.05000\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 278s 70ms/step - loss: 3.9225 - acc: 0.0150 - f1: 0.0000e+00 - val_loss: 3.8787 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.05000\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 279s 70ms/step - loss: 3.9222 - acc: 0.0118 - f1: 0.0000e+00 - val_loss: 3.8823 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.05000\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 278s 69ms/step - loss: 3.9218 - acc: 0.0112 - f1: 0.0000e+00 - val_loss: 3.8857 - val_acc: 0.0500 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.05000\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "scores = []\n",
    "num_validation_sample = len(x)//k\n",
    "for i in range(k):\n",
    "    print(\"-----------------------------\")\n",
    "    print('Processing fold #', i)\n",
    "    print(\"-----------------------------\")\n",
    "    val_data = x[i * num_validation_sample: (i + 1) * num_validation_sample]\n",
    "    val_lab  = y[i * num_validation_sample: (i + 1) * num_validation_sample]\n",
    "    \n",
    "    parttial_train_X_data = np.concatenate(\n",
    "                [x[:i * num_validation_sample],\n",
    "                x[(i + 1) * num_validation_sample:]], axis=0)\n",
    "    \n",
    "    parttial_train_X_label = np.concatenate(\n",
    "                [y[:i * num_validation_sample],\n",
    "                y[(i + 1) * num_validation_sample:]], axis=0)\n",
    "    \n",
    "    history_imbd62_SGDW = model_SGDW.fit(parttial_train_X_data, parttial_train_X_label, \n",
    "                               validation_data=(val_data,val_lab),\n",
    "                               batch_size=batch_size,epochs=num_epochs,verbose=1,\n",
    "                               callbacks=[reduce_lr_sgd,earlystop,csv_logger,checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print average acc\n",
    "average_acc = np.mean(history_imbd62_SGDW.history['acc'])\n",
    "print(average_acc)\n",
    "print(\"------------\")\n",
    "#Print average val_acc\n",
    "average_val_acc = np.mean(history_imbd62_SGDW.history['val_acc'])\n",
    "print(average_val_acc)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print average loss\n",
    "average_loss = np.mean(history_imbd62_SGDW.history['loss'])\n",
    "print(average_loss)\n",
    "print(\"------------\")\n",
    "#Print average val_loss\n",
    "average_val_loss = np.mean(history_imbd62_SGDW.history['val_loss'])\n",
    "print(average_val_loss)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print average f1-score\n",
    "average_f1 = np.mean(history_imbd62_SGDW.history['f1'])\n",
    "print(average_f1)\n",
    "print(\"------------\")\n",
    "#Print average val_f1-score\n",
    "average_val_f1 = np.mean(history_imbd62_SGDW.history['val_f1'])\n",
    "print(average_val_f1)\n",
    "print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model on the 20% Test Dataset\n",
    "scorev =  model_SGDW.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss: ', scorev[0])\n",
    "print('Test accuracy:', scorev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_value = model_SGDW.predict(x_test)\n",
    "predict_class = np.argmax(prediction_value, axis=-1)\n",
    "y_test_integer = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Detailed analysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test_integer, predict_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsource = '../../save_models/imbd62/'\n",
    "df_movies_SGD = pd.read_csv(modelsource +'/imbd62_SGDW.csv',\n",
    "                        engine='python',\n",
    "                        sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_SGD.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_movies.index, df_movies['acc'], 'go-', color='red', label= 'acc',linewidth=3)\n",
    "plt.plot(df_movies.index, df_movies['val_acc'], '--^', color='blue',label= 'val_acc',linewidth=3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuarcy')\n",
    "plt.savefig(modelsource + 'imdb62_SGDW_acc.png')\n",
    "plt.title('AT-BLSTM model accouracy for IMBD62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_movies.index, df_movies['loss'], 'go-', color='red', label= 'loss',linewidth=3)\n",
    "plt.plot(df_movies.index, df_movies['val_loss'], '--^', color='blue',label= 'val_loss',linewidth=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(modelsource + 'imdb62_SGDW_loss.png')\n",
    "plt.title('AT-BLSTM model loss for IMBD62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_movies.index, df_movies['f1'], 'go-', color='red', label= 'f1',linewidth=3)\n",
    "plt.plot(df_movies.index, df_movies['val_f1'], '--*', color='blue',label= 'val_f1',linewidth=3)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-score')\n",
    "plt.savefig(modelsource + 'imdb62_SGDW_F1-score.png')\n",
    "plt.title('AT-BLSTM model F-score for IMBD62')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD without earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SGD = build_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SGD.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=SGD(momentum=0.9),\n",
    "                  metrics=['accuracy',f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_SGD.save_weights(model_save_dir+\"saved_IMBD62_SGD_model.h5\")\n",
    "history_imbd62_atten = model_SGD.fit(x_train, y_train, \n",
    "                            validation_data=(x_test,y_test),\n",
    "                            batch_size=512,epochs=80,\n",
    "                            verbose=1,callbacks=[reduce_lr_adam,csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AttentionBidirectionalLSTMforIMDb62.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
